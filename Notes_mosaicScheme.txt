Notes on the scheme for a parallel mosaicing tool.

- Similar in spirit to procCopy.py. More complicated, because of
  multiple input files, which only overlap in some areas.
- Over all inputs, work out the pixel grid of the output raster
  (i.e. projection, geoTransform, nrows, ncols)
  - Note that this involves opening every input to get basic raster info.
    In principle, this could be done in parallel, creating something like
    a dictionary of ImageInfo objects, keyed by filename
  - Should we cope with different input projections? We could generate
    on-the-fly VRT files to reproject into the output grid.
- Divide this output grid into blocks, and make a blockList
- For each block in list, work out which input files it intersects,
  and for each of those files, work out the block of data corresponding
  to the current block of output. Keep this as the block list annotated
  with input blocks
- Turn this into a much longer list of blocks which need to be read. Keep
  the order so that input blocks for the same output block are grouped
  together, and these groups are then in the order in which we would
  write the output blocks. This ordering is critical to the parallel
  performance, so the docstring should describe in detail what is occurring
  and why.
- Spin off <n> read workers, each running the same readFunc. Pass in a
  subset of the big block list (every n-th block, similar to procCopy.py),
  where, as a block is read, it is placed in the main queue, along with
  its output and input block specification.
  The construction of the sub-list of blocks for the n-th worker to read
  is critical to the parallelism available. While it is only one line
  of code, it should be a separate function, so it can have a huge
  docstring to explain how important it is.
- In main thread, run the writeFunc. Takes the original block list with
  annotated input blocks. Its main loop is similar to procCopy.py. Note that
  this scheme is designed to write the blocks in normal block order, to
  avoid random sequencing of blocks which may ruin other caching schemes
  when using the file later on.
  - start at output block 0
  - while not finished:
      - check for anything to get from the queue, and put it
        into local cache, keyed by output block spec, and also the filename
        it came from.
      - check local cache for all input blocks for the current output
        block. If all inputs are present in cache, then combine as desired,
        and write output block to outfile. Remove input blocks from cache,
        and go to next output block.
      - Should include some sort of guard to prevent the local cache from
        growing too large. There is potential for a deadlock to arise,
        though, so this should be approached with caution.
- The above is for a single band, but can be looped for multi-band files.
  Each band would be independent of the others, and completed before the
  next one starts.


Extra notes
-----------
* If using threads, I think (??) that there should only be a single GDAL
  file/block cache, which should cope with partial block reads when output
  grid does not align with input grid for some file. However, this would
  not be true if using processes, so this is an argument against processes.
* As new files are opened, we should cache the Dataset & Band objects, so
  they are only opened once. Can we share this between all threads? Yes, but
  only if we lock it with semaphores. 
* For each file we should also keep record of which blocks need to be read,
  and which have been read, so that we can close the file once all blocks
  have been read for that file.
* I have now demonstrated that queue.Queue (used between threads) sends
  objects by reference, i.e. no copying, whereas multitasking's equivalent
  Queue class (used between processes) does a pickle-copy-unpickle.
  Another reason to use threads instead of processes.
* GDAL's notes about multi-threading are very clear that any GDAL data 
  structures should not be shared between threads, and explicitly mentions
  that Dataset objects should not be shared. This suggests that I
  should allow each thread to have its own open Dataset object for any
  given file. That means the cache of open GDAL objects should also be
  private for each thread. 
